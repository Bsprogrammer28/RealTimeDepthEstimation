{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxpstZnRjIWu"
      },
      "source": [
        "# Install necessary packages\n",
        "\n",
        "If you run this notebook on Google Colab, you'll have to install timm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqvdi6ZdjZWF"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "l--K9BQ9aCUI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from IPython.display import Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8ZicTxDjdNx"
      },
      "source": [
        "# Load MiDaS model.\n",
        "\n",
        "We'll load MiDaS thanks to torch hub. Feel free to use different versions of the model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0saVCNgVaCcf",
        "outputId": "aea4dd61-43c6-403b-d214-68e7fe32ac16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\bhave/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
            "c:\\Users\\bhave\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\_factory.py:126: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DPT Hybrid model successfully loaded.\n"
          ]
        }
      ],
      "source": [
        "# Let's load a DPT Hybrid model for depth estimation task.\n",
        "model_type = \"DPT_Hybrid\"  # DPT Hybrid model\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "\n",
        "# Move the model to GPU if it is available.\n",
        "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device_name)\n",
        "midas.to(device)\n",
        "print('DPT Hybrid model successfully loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViyPqoWObMdW"
      },
      "source": [
        "## Load data and set image preprocessor class:\n",
        "\n",
        "We need to load, resize and normalize our images so that MiDaS can process them correctly. Fortunately, MiDaS has a ``transforms`` that does the preprocessing for us. You can see the video whoose depth we're going to estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KmVP7TqmbQji",
        "outputId": "97b44c64-0260-4a25-fe11-43ec5d56b219"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\bhave/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "# Use transforms class\n",
        "transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "transform = transforms.dpt_transform  # If you used a small midas model,\n",
        "                                      # use transforms.small_transform instead.\n",
        "\n",
        "# Open the camera (usually the default camera is at index 0)\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open camera.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "765DrX9cjoKG"
      },
      "source": [
        "Now we are going to extract all of the images from the video and place them in a folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MZ1LZ4pj1PF",
        "outputId": "b5c77f72-0caf-4ffe-c2af-0f6adb4339bf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Output directory for JPEG images\n",
        "# output_directory = f\"./data/video_into_imagesbw\"\n",
        "\n",
        "# # Ensure that the output directory exists\n",
        "# os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# # Open the video file\n",
        "# cap = cv2.VideoCapture(\"data\\interior_designbw.mp4\")\n",
        "\n",
        "# # Get the resolution of the video\n",
        "# frame_width = int(cap.get(3))  # Width\n",
        "# frame_height = int(cap.get(4))  # Height\n",
        "\n",
        "# # Check if the video was opened successfully\n",
        "# if not cap.isOpened():\n",
        "#     print(\"Unable to open the video. Check the file path.\")\n",
        "# else:\n",
        "#     frame_count = 0\n",
        "\n",
        "#     while True:\n",
        "#         ret, frame = cap.read()\n",
        "\n",
        "#         # Check if reading the frame was successful\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#           # Save the frame as a JPEG image\n",
        "#         frame_filename = os.path.join(output_directory, f'frame_{frame_count:04d}.jpg')\n",
        "#         cv2.imwrite(frame_filename, frame)\n",
        "#         frame_count += 1\n",
        "\n",
        "#     # Release the video file\n",
        "#     cap.release()\n",
        "\n",
        "#     print(f'{frame_count} images have been extracted and saved in {output_directory}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time: 33.31s\n",
            "Average time per frame: 1.28s\n"
          ]
        }
      ],
      "source": [
        "midas.eval()\n",
        "\n",
        "total_time = []\n",
        "depth_video = []\n",
        "\n",
        "# Define the sharpening kernel\n",
        "sharpening_kernel = np.array([[0, -1, 0],\n",
        "                              [-1, 5,-1],\n",
        "                              [0, -1, 0]])\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Could not read frame.\")\n",
        "        break\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply the sharpening filter\n",
        "    gray_frame = cv2.filter2D(gray_frame, -1, sharpening_kernel)\n",
        "\n",
        "    # Convert grayscale image to 3-channel image\n",
        "    gray_frame_3ch = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Apply transforms:\n",
        "    input_img = transform(gray_frame_3ch).to(device)\n",
        "\n",
        "    # Prediction and preprocess output:\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        pred = midas(input_img)\n",
        "\n",
        "        pred = torch.nn.functional.interpolate(pred.unsqueeze(1),\n",
        "                                               size=gray_frame.shape[:2],\n",
        "                                               mode=\"bicubic\",\n",
        "                                               align_corners=False,\n",
        "                                               ).squeeze()\n",
        "        depth_img = pred.cpu().numpy()\n",
        "\n",
        "        # Apply Gaussian blur to smooth the depth map\n",
        "        depth_img = cv2.GaussianBlur(depth_img, (3, 3), 0)\n",
        "\n",
        "        # Resizing and coloring the image\n",
        "        depth_img = cv2.normalize(depth_img, None, 0, 1, norm_type=cv2.NORM_MINMAX,\n",
        "                                  dtype=cv2.CV_64F)\n",
        "        depth_img = (depth_img * 255).astype(np.uint8)\n",
        "        depth_img = cv2.applyColorMap(depth_img, cv2.COLORMAP_MAGMA)\n",
        "\n",
        "        end = time.time()\n",
        "        total_time.append(end - start)\n",
        "        depth_video.append(depth_img)\n",
        "\n",
        "        # Concatenate the original frame and the depth image side by side\n",
        "        combined_img = np.hstack((cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR), depth_img))\n",
        "\n",
        "        # Display the combined frame\n",
        "        cv2.imshow('Original and Depth Estimation', combined_img)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "print(f\"Total time: {np.sum(total_time):.2f}s\")\n",
        "print(f\"Average time per frame: {np.mean(total_time):.2f}s\")\n",
        "\n",
        "# Release the camera and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19bh6SgakNIn"
      },
      "source": [
        "# Run the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfTUJXx8kPLp"
      },
      "source": [
        "Now that we have decomposed the video into images, we can run the model for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKqUF07HaCiZ",
        "outputId": "ed6f6f26-bf41-4069-e6c2-4159bcac2a62"
      },
      "outputs": [],
      "source": [
        "# # Inference mode\n",
        "# midas.eval()\n",
        "\n",
        "# total_time = []\n",
        "# depth_video = []\n",
        "\n",
        "# for i in range(frame_count):\n",
        "\n",
        "#   FRAME_PATH = f\"./data/video_into_imagesbw/frame_{i:04d}.jpg\"\n",
        "#   img = cv2.imread(FRAME_PATH, cv2.IMREAD_COLOR)\n",
        "\n",
        "#   # Apply transforms:\n",
        "#   input_img = transform(img).to(device)\n",
        "\n",
        "#   # Prediction and preprocess output:\n",
        "#   with torch.no_grad():\n",
        "#     start = time.time()\n",
        "#     pred = midas(input_img)\n",
        "\n",
        "#     pred = torch.nn.functional.interpolate(pred.unsqueeze(1),\n",
        "#                                           size = img.shape[:2],\n",
        "#                                           mode = \"bicubic\",\n",
        "#                                           align_corners = False,\n",
        "#                                           ).squeeze()\n",
        "#     depth_img = pred.cpu().numpy()\n",
        "\n",
        "#     #Resizing and coloring the image\n",
        "#     depth_img = cv2.normalize(depth_img, None, 0, 1, norm_type = cv2.NORM_MINMAX,\n",
        "#                               dtype = cv2.CV_64F)\n",
        "#     depth_img = (depth_img*255).astype(np.uint8)\n",
        "#     depth_img = cv2.applyColorMap(depth_img, cv2.COLORMAP_MAGMA)\n",
        "\n",
        "#     end = time.time()\n",
        "#     total_time.append(end - start)\n",
        "#     depth_video.append(depth_img)\n",
        "\n",
        "# print(f\"Total time: {np.sum(total_time):.2f}s\")\n",
        "# print(f\"Average time per image: {np.mean(total_time):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCaV223zkjFq"
      },
      "source": [
        "As we can see, the ``DPT_Hybrid`` model needs an everage of 0.15s inference + processing time, which is approximately 7 frames per second in the context of a real time depth estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFa64g6KlnVG"
      },
      "source": [
        "# Results\n",
        "\n",
        "Now let's see the video !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "SCSu4-i6nHON"
      },
      "outputs": [],
      "source": [
        "# frame_size = (1920, 1080)  # Specify the width and height of your frames\n",
        "# frame_rate = 30  # Frames per second\n",
        "# codec = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 format\n",
        "\n",
        "# output_file = 'output_videobw.mp4'\n",
        "# out = cv2.VideoWriter(output_file, codec, frame_rate, frame_size)\n",
        "\n",
        "# for frame in depth_video:  # 'frames' is your list of RGB frames\n",
        "#     out.write(frame)\n",
        "\n",
        "# out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172,
          "resources": {
            "http://localhost:8080/output_video.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "etKX9pDnn2NA",
        "outputId": "b3b3bcde-c217-4401-adfd-f009b54c6bf8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'output_file' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Video(\u001b[43moutput_file\u001b[49m, embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'output_file' is not defined"
          ]
        }
      ],
      "source": [
        "Video(output_file, embed = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-smFHCXejHJr"
      },
      "source": [
        "As we can see, the results on the video are quite detailed for a monocular camera depth estimation!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
